{"version":3,"file":"operations.js","sourceRoot":"","sources":["../../../src/api/operations.ts"],"names":[],"mappings":"AAAA,uCAAuC;AACvC,kCAAkC;AAIlC,OAAO,EAUL,YAAY,GACb,MAAM,kBAAkB,CAAC;AAwK1B,MAAM,UAAU,kBAAkB,CAChC,OAAe,EACf,KAAwB,EACxB,YAAoB,EACpB,UAAgC,EAAE,cAAc,EAAE,EAAE,EAAE;;IAEtD,OAAO,OAAO,CAAC,IAAI,CAAC,wCAAwC,EAAE,YAAY,CAAC,CAAC,IAAI,CAAC;QAC/E,uBAAuB,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,uBAAuB;QACxE,eAAe,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,eAAe;QACxD,OAAO,oBAAO,MAAA,OAAO,CAAC,cAAc,0CAAE,OAAO,CAAE;QAC/C,IAAI,EAAE,EAAE,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI,EAAE,KAAK,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,KAAK,EAAE,KAAK,EAAE,KAAK,EAAE;KACnE,CAAC,CAAC;AACL,CAAC;AAED,MAAM,CAAC,KAAK,UAAU,yBAAyB,CAC7C,MAA+D;;IAE/D,IAAI,YAAY,CAAC,MAAM,CAAC,EAAE;QACxB,MAAM,MAAM,CAAC,IAAI,CAAC;KACnB;IAED,OAAO;QACL,IAAI,EAAE,CAAC,MAAA,MAAM,CAAC,IAAI,CAAC,MAAM,CAAC,mCAAI,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;YAC5C,SAAS,EAAE,CAAC,CAAC,WAAW,CAAC;YACzB,KAAK,EAAE,CAAC,CAAC,OAAO,CAAC;SAClB,CAAC,CAAC;QACH,KAAK,EAAE;YACL,YAAY,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,eAAe,CAAC;YAChD,WAAW,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,cAAc,CAAC;SAC/C;KACF,CAAC;AACJ,CAAC;AAED,gDAAgD;AAChD,MAAM,CAAC,KAAK,UAAU,aAAa,CACjC,OAAe,EACf,KAAwB,EACxB,YAAoB,EACpB,UAAgC,EAAE,cAAc,EAAE,EAAE,EAAE;IAEtD,MAAM,MAAM,GAAG,MAAM,kBAAkB,CAAC,OAAO,EAAE,KAAK,EAAE,YAAY,EAAE,OAAO,CAAC,CAAC;IAC/E,OAAO,yBAAyB,CAAC,MAAM,CAAC,CAAC;AAC3C,CAAC;AAED,MAAM,UAAU,mBAAmB,CACjC,OAAe,EACf,MAAgB,EAChB,YAAoB,EACpB,UAAiC,EAAE,cAAc,EAAE,EAAE,EAAE;;IAEvD,OAAO,OAAO,CAAC,IAAI,CAAC,yCAAyC,EAAE,YAAY,CAAC,CAAC,IAAI,CAAC;QAChF,uBAAuB,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,uBAAuB;QACxE,eAAe,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,eAAe;QACxD,OAAO,oBAAO,MAAA,OAAO,CAAC,cAAc,0CAAE,OAAO,CAAE;QAC/C,IAAI,EAAE;YACJ,MAAM,EAAE,MAAM;YACd,UAAU,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,SAAS;YAC9B,WAAW,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,WAAW;YACjC,KAAK,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACpB,UAAU,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,SAAS;YAC9B,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACnB,CAAC,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,CAAC;YACb,QAAQ,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,QAAQ;YAC3B,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACnB,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACnB,gBAAgB,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,eAAe;YAC1C,iBAAiB,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,gBAAgB;YAC5C,OAAO,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,MAAM;YACxB,MAAM,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,MAAM;YACvB,KAAK,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,KAAK;SACtB;KACF,CAAC,CAAC;AACL,CAAC;AAED,MAAM,CAAC,KAAK,UAAU,0BAA0B,CAC9C,MAAiE;;IAEjE,IAAI,YAAY,CAAC,MAAM,CAAC,EAAE;QACxB,MAAM,MAAM,CAAC,IAAI,CAAC;KACnB;IAED,OAAO;QACL,EAAE,EAAE,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC;QACrB,OAAO,EAAE,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC;QAC/B,OAAO,EAAE,CAAC,MAAA,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC,mCAAI,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;YAClD,IAAI,EAAE,CAAC,CAAC,MAAM,CAAC;YACf,KAAK,EAAE,CAAC,CAAC,OAAO,CAAC;YACjB,QAAQ,EACN,CAAC,CAAC,QAAQ,KAAK,IAAI;gBACjB,CAAC,CAAC,IAAI;gBACN,CAAC,CAAC;oBACE,MAAM,EAAE,CAAC,CAAC,QAAQ,CAAC,QAAQ,CAAC;oBAC5B,aAAa,EAAE,CAAC,CAAC,QAAQ,CAAC,gBAAgB,CAAC;oBAC3C,WAAW,EAAE,CAAC,CAAC,QAAQ,CAAC,cAAc,CAAC;oBACvC,UAAU,EAAE,CAAC,CAAC,QAAQ,CAAC,aAAa,CAAC;iBACtC;YACP,YAAY,EAAE,CAAC,CAAC,eAAe,CAAC;SACjC,CAAC,CAAC;QACH,KAAK,EAAE;YACL,gBAAgB,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,mBAAmB,CAAC;YACxD,YAAY,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,eAAe,CAAC;YAChD,WAAW,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,cAAc,CAAC;SAC/C;KACF,CAAC;AACJ,CAAC;AAED;;;;GAIG;AACH,MAAM,CAAC,KAAK,UAAU,cAAc,CAClC,OAAe,EACf,MAAgB,EAChB,YAAoB,EACpB,UAAiC,EAAE,cAAc,EAAE,EAAE,EAAE;IAEvD,MAAM,MAAM,GAAG,MAAM,mBAAmB,CAAC,OAAO,EAAE,MAAM,EAAE,YAAY,EAAE,OAAO,CAAC,CAAC;IACjF,OAAO,0BAA0B,CAAC,MAAM,CAAC,CAAC;AAC5C,CAAC;AAED,MAAM,UAAU,uBAAuB,CACrC,OAAe,EACf,QAAuB,EACvB,YAAoB,EACpB,UAAqC,EAAE,cAAc,EAAE,EAAE,EAAE;;IAE3D,OAAO,OAAO,CAAC,IAAI,CAAC,8CAA8C,EAAE,YAAY,CAAC,CAAC,IAAI,CAAC;QACrF,uBAAuB,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,uBAAuB;QACxE,eAAe,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,eAAe;QACxD,OAAO,oBAAO,MAAA,OAAO,CAAC,cAAc,0CAAE,OAAO,CAAE;QAC/C,IAAI,EAAE;YACJ,QAAQ,EAAE,QAAQ;YAClB,UAAU,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,SAAS;YAC9B,WAAW,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,WAAW;YACjC,KAAK,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACpB,UAAU,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,SAAS;YAC9B,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACnB,CAAC,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,CAAC;YACb,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACnB,gBAAgB,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,eAAe;YAC1C,iBAAiB,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,gBAAgB;YAC5C,MAAM,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,MAAM;YACvB,KAAK,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,KAAK;SACtB;KACF,CAAC,CAAC;AACL,CAAC;AAED,MAAM,CAAC,KAAK,UAAU,8BAA8B,CAClD,MAAyE;;IAEzE,IAAI,YAAY,CAAC,MAAM,CAAC,EAAE;QACxB,MAAM,MAAM,CAAC,IAAI,CAAC;KACnB;IAED,OAAO;QACL,EAAE,EAAE,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC;QACrB,OAAO,EAAE,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC;QAC/B,OAAO,EAAE,CAAC,MAAA,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC,mCAAI,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE;;YAAC,OAAA,CAAC;gBAClD,OAAO,EAAE,CAAC,CAAC,CAAC,OAAO;oBACjB,CAAC,CAAC,SAAS;oBACX,CAAC,CAAC,EAAE,IAAI,EAAE,MAAA,CAAC,CAAC,OAAO,0CAAG,MAAM,CAAC,EAAE,OAAO,EAAE,MAAA,CAAC,CAAC,OAAO,0CAAG,SAAS,CAAC,EAAE;gBAClE,KAAK,EAAE,CAAC,CAAC,OAAO,CAAC;gBACjB,YAAY,EAAE,CAAC,CAAC,eAAe,CAAC;gBAChC,KAAK,EAAE,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,EAAE,IAAI,EAAE,MAAA,CAAC,CAAC,KAAK,0CAAG,MAAM,CAAC,EAAE,OAAO,EAAE,MAAA,CAAC,CAAC,KAAK,0CAAG,SAAS,CAAC,EAAE;aACzF,CAAC,CAAA;SAAA,CAAC;QACH,KAAK,EAAE;YACL,gBAAgB,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,mBAAmB,CAAC;YACxD,YAAY,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,eAAe,CAAC;YAChD,WAAW,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,cAAc,CAAC;SAC/C;KACF,CAAC;AACJ,CAAC;AAED;;;;GAIG;AACH,MAAM,CAAC,KAAK,UAAU,kBAAkB,CACtC,OAAe,EACf,QAAuB,EACvB,YAAoB,EACpB,UAAqC,EAAE,cAAc,EAAE,EAAE,EAAE;IAE3D,MAAM,MAAM,GAAG,MAAM,uBAAuB,CAAC,OAAO,EAAE,QAAQ,EAAE,YAAY,EAAE,OAAO,CAAC,CAAC;IACvF,OAAO,8BAA8B,CAAC,MAAM,CAAC,CAAC;AAChD,CAAC;AAED,MAAM,UAAU,oBAAoB,CAAC,IAAyB;;IAC5D,OAAO;QACL,EAAE,EAAE,IAAI,CAAC,IAAI,CAAC;QACd,OAAO,EAAE,IAAI,CAAC,SAAS,CAAC;QACxB,OAAO,EAAE,CAAC,MAAA,IAAI,CAAC,SAAS,CAAC,mCAAI,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAe,EAAE,EAAE,CAAC,CAAC;YACzD,IAAI,EAAE,CAAC,CAAC,MAAM,CAAC;YACf,KAAK,EAAE,CAAC,CAAC,OAAO,CAAC;YACjB,QAAQ,EACN,CAAC,CAAC,QAAQ,KAAK,IAAI;gBACjB,CAAC,CAAC,IAAI;gBACN,CAAC,CAAC;oBACE,MAAM,EAAE,CAAC,CAAC,QAAQ,CAAC,QAAQ,CAAC;oBAC5B,aAAa,EAAE,CAAC,CAAC,QAAQ,CAAC,gBAAgB,CAAC;oBAC3C,WAAW,EAAE,CAAC,CAAC,QAAQ,CAAC,cAAc,CAAC;oBACvC,UAAU,EAAE,CAAC,CAAC,QAAQ,CAAC,aAAa,CAAC;iBACtC;YACP,YAAY,EAAE,CAAC,CAAC,eAAe,CAAC;SACjC,CAAC,CAAC;KACJ,CAAC;AACJ,CAAC;AAED,MAAM,UAAU,wBAAwB,CACtC,IAAyB;;IAEzB,OAAO;QACL,EAAE,EAAE,IAAI,CAAC,IAAI,CAAC;QACd,OAAO,EAAE,IAAI,CAAC,SAAS,CAAC;QACxB,OAAO,EAAE,CAAC,MAAA,IAAI,CAAC,SAAS,CAAC,mCAAI,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAmB,EAAE,EAAE;;YAAC,OAAA,CAAC;gBAC7D,OAAO,EAAE,CAAC,CAAC,CAAC,OAAO;oBACjB,CAAC,CAAC,SAAS;oBACX,CAAC,CAAC,EAAE,IAAI,EAAE,MAAA,CAAC,CAAC,OAAO,0CAAG,MAAM,CAAC,EAAE,OAAO,EAAE,MAAA,CAAC,CAAC,OAAO,0CAAG,SAAS,CAAC,EAAE;gBAClE,KAAK,EAAE,CAAC,CAAC,OAAO,CAAC;gBACjB,YAAY,EAAE,CAAC,CAAC,eAAe,CAAC;gBAChC,KAAK,EAAE,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,EAAE,IAAI,EAAE,MAAA,CAAC,CAAC,KAAK,0CAAG,MAAM,CAAC,EAAE,OAAO,EAAE,MAAA,CAAC,CAAC,KAAK,0CAAG,SAAS,CAAC,EAAE;aACzF,CAAC,CAAA;SAAA,CAAC;KACJ,CAAC;AACJ,CAAC","sourcesContent":["// Copyright (c) Microsoft Corporation.\n// Licensed under the MIT license.\n\nimport { StreamableMethod } from \"@azure-rest/core-client\";\nimport { RequestOptions } from \"../common/interfaces.js\";\nimport {\n  ChatChoiceOutput,\n  ChoiceOutput,\n  OpenAIContext as Client,\n  GetChatCompletions200Response,\n  GetChatCompletionsDefaultResponse,\n  GetCompletions200Response,\n  GetCompletionsDefaultResponse,\n  GetEmbeddings200Response,\n  GetEmbeddingsDefaultResponse,\n  isUnexpected,\n} from \"../rest/index.js\";\nimport { ChatCompletions, ChatMessage, Completions, Embeddings } from \"./models.js\";\n\nexport interface GetEmbeddingsOptions extends RequestOptions {\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The model name to provide as part of this embeddings request.\n   * Not applicable to Azure OpenAI, where deployment information should be included in the Azure\n   * resource URI that's connected to.\n   */\n  model?: string;\n}\n\nexport interface GetCompletionsOptions extends RequestOptions {\n  /** The maximum number of tokens to generate. */\n  maxTokens?: number;\n  /**\n   * The sampling temperature to use that controls the apparent creativity of generated completions.\n   * Higher values will make output more random while lower values will make results more focused\n   * and deterministic.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  temperature?: number;\n  /**\n   * An alternative to sampling with temperature called nucleus sampling. This value causes the\n   * model to consider the results of tokens with the provided probability mass. As an example, a\n   * value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be\n   * considered.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  topP?: number;\n  /**\n   * A map between GPT token IDs and bias scores that influences the probability of specific tokens\n   * appearing in a completions response. Token IDs are computed via external tokenizer tools, while\n   * bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to\n   * a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias\n   * score varies by model.\n   */\n  logitBias?: Record<string, number>;\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The number of completions choices that should be generated per provided prompt as part of an\n   * overall completions response.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for max_tokens and stop.\n   */\n  n?: number;\n  /**\n   * A value that controls the emission of log probabilities for the provided number of most likely\n   * tokens within a completions response.\n   */\n  logprobs?: number;\n  /**\n   * A value specifying whether completions responses should include input prompts as prefixes to\n   * their generated output.\n   */\n  echo?: boolean;\n  /** A collection of textual sequences that will end completions generation. */\n  stop?: string[];\n  /**\n   * A value that influences the probability of generated tokens appearing based on their existing\n   * presence in generated text.\n   * Positive values will make tokens less likely to appear when they already exist and increase the\n   * model's likelihood to output new topics.\n   */\n  presencePenalty?: number;\n  /**\n   * A value that influences the probability of generated tokens appearing based on their cumulative\n   * frequency in generated text.\n   * Positive values will make tokens less likely to appear as their frequency increases and\n   * decrease the likelihood of the model repeating the same statements verbatim.\n   */\n  frequencyPenalty?: number;\n  /**\n   * A value that controls how many completions will be internally generated prior to response\n   * formulation.\n   * When used together with n, best_of controls the number of candidate completions and must be\n   * greater than n.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for max_tokens and stop.\n   */\n  bestOf?: number;\n  /** A value indicating whether chat completions should be streamed for this request. */\n  stream?: boolean;\n  /**\n   * The model name to provide as part of this completions request.\n   * Not applicable to Azure OpenAI, where deployment information should be included in the Azure\n   * resource URI that's connected to.\n   */\n  model?: string;\n}\n\nexport interface GetChatCompletionsOptions extends RequestOptions {\n  /** The maximum number of tokens to generate. */\n  maxTokens?: number;\n  /**\n   * The sampling temperature to use that controls the apparent creativity of generated completions.\n   * Higher values will make output more random while lower values will make results more focused\n   * and deterministic.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  temperature?: number;\n  /**\n   * An alternative to sampling with temperature called nucleus sampling. This value causes the\n   * model to consider the results of tokens with the provided probability mass. As an example, a\n   * value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be\n   * considered.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  topP?: number;\n  /**\n   * A map between GPT token IDs and bias scores that influences the probability of specific tokens\n   * appearing in a completions response. Token IDs are computed via external tokenizer tools, while\n   * bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to\n   * a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias\n   * score varies by model.\n   */\n  logitBias?: Record<string, number>;\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The number of chat completions choices that should be generated for a chat completions\n   * response.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for max_tokens and stop.\n   */\n  n?: number;\n  /** A collection of textual sequences that will end completions generation. */\n  stop?: string[];\n  /**\n   * A value that influences the probability of generated tokens appearing based on their existing\n   * presence in generated text.\n   * Positive values will make tokens less likely to appear when they already exist and increase the\n   * model's likelihood to output new topics.\n   */\n  presencePenalty?: number;\n  /**\n   * A value that influences the probability of generated tokens appearing based on their cumulative\n   * frequency in generated text.\n   * Positive values will make tokens less likely to appear as their frequency increases and\n   * decrease the likelihood of the model repeating the same statements verbatim.\n   */\n  frequencyPenalty?: number;\n  /** A value indicating whether chat completions should be streamed for this request. */\n  stream?: boolean;\n  /**\n   * The model name to provide as part of this completions request.\n   * Not applicable to Azure OpenAI, where deployment information should be included in the Azure\n   * resource URI that's connected to.\n   */\n  model?: string;\n}\n\nexport function _getEmbeddingsSend(\n  context: Client,\n  input: string | string[],\n  deploymentId: string,\n  options: GetEmbeddingsOptions = { requestOptions: {} }\n): StreamableMethod<GetEmbeddings200Response | GetEmbeddingsDefaultResponse> {\n  return context.path(\"/deployments/{deploymentId}/embeddings\", deploymentId).post({\n    allowInsecureConnection: options.requestOptions?.allowInsecureConnection,\n    skipUrlEncoding: options.requestOptions?.skipUrlEncoding,\n    headers: { ...options.requestOptions?.headers },\n    body: { user: options?.user, model: options?.model, input: input },\n  });\n}\n\nexport async function _getEmbeddingsDeserialize(\n  result: GetEmbeddings200Response | GetEmbeddingsDefaultResponse\n): Promise<Embeddings> {\n  if (isUnexpected(result)) {\n    throw result.body;\n  }\n\n  return {\n    data: (result.body[\"data\"] ?? []).map((p) => ({\n      embedding: p[\"embedding\"],\n      index: p[\"index\"],\n    })),\n    usage: {\n      promptTokens: result.body.usage[\"prompt_tokens\"],\n      totalTokens: result.body.usage[\"total_tokens\"],\n    },\n  };\n}\n\n/** Return the embeddings for a given prompt. */\nexport async function getEmbeddings(\n  context: Client,\n  input: string | string[],\n  deploymentId: string,\n  options: GetEmbeddingsOptions = { requestOptions: {} }\n): Promise<Embeddings> {\n  const result = await _getEmbeddingsSend(context, input, deploymentId, options);\n  return _getEmbeddingsDeserialize(result);\n}\n\nexport function _getCompletionsSend(\n  context: Client,\n  prompt: string[],\n  deploymentId: string,\n  options: GetCompletionsOptions = { requestOptions: {} }\n): StreamableMethod<GetCompletions200Response | GetCompletionsDefaultResponse> {\n  return context.path(\"/deployments/{deploymentId}/completions\", deploymentId).post({\n    allowInsecureConnection: options.requestOptions?.allowInsecureConnection,\n    skipUrlEncoding: options.requestOptions?.skipUrlEncoding,\n    headers: { ...options.requestOptions?.headers },\n    body: {\n      prompt: prompt,\n      max_tokens: options?.maxTokens,\n      temperature: options?.temperature,\n      top_p: options?.topP,\n      logit_bias: options?.logitBias,\n      user: options?.user,\n      n: options?.n,\n      logprobs: options?.logprobs,\n      echo: options?.echo,\n      stop: options?.stop,\n      presence_penalty: options?.presencePenalty,\n      frequency_penalty: options?.frequencyPenalty,\n      best_of: options?.bestOf,\n      stream: options?.stream,\n      model: options?.model,\n    },\n  });\n}\n\nexport async function _getCompletionsDeserialize(\n  result: GetCompletions200Response | GetCompletionsDefaultResponse\n): Promise<Completions> {\n  if (isUnexpected(result)) {\n    throw result.body;\n  }\n\n  return {\n    id: result.body[\"id\"],\n    created: result.body[\"created\"],\n    choices: (result.body[\"choices\"] ?? []).map((p) => ({\n      text: p[\"text\"],\n      index: p[\"index\"],\n      logprobs:\n        p.logprobs === null\n          ? null\n          : {\n              tokens: p.logprobs[\"tokens\"],\n              tokenLogprobs: p.logprobs[\"token_logprobs\"],\n              topLogprobs: p.logprobs[\"top_logprobs\"],\n              textOffset: p.logprobs[\"text_offset\"],\n            },\n      finishReason: p[\"finish_reason\"],\n    })),\n    usage: {\n      completionTokens: result.body.usage[\"completion_tokens\"],\n      promptTokens: result.body.usage[\"prompt_tokens\"],\n      totalTokens: result.body.usage[\"total_tokens\"],\n    },\n  };\n}\n\n/**\n * Gets completions for the provided input prompts.\n * Completions support a wide variety of tasks and generate text that continues from or \"completes\"\n * provided prompt data.\n */\nexport async function getCompletions(\n  context: Client,\n  prompt: string[],\n  deploymentId: string,\n  options: GetCompletionsOptions = { requestOptions: {} }\n): Promise<Completions> {\n  const result = await _getCompletionsSend(context, prompt, deploymentId, options);\n  return _getCompletionsDeserialize(result);\n}\n\nexport function _getChatCompletionsSend(\n  context: Client,\n  messages: ChatMessage[],\n  deploymentId: string,\n  options: GetChatCompletionsOptions = { requestOptions: {} }\n): StreamableMethod<GetChatCompletions200Response | GetChatCompletionsDefaultResponse> {\n  return context.path(\"/deployments/{deploymentId}/chat/completions\", deploymentId).post({\n    allowInsecureConnection: options.requestOptions?.allowInsecureConnection,\n    skipUrlEncoding: options.requestOptions?.skipUrlEncoding,\n    headers: { ...options.requestOptions?.headers },\n    body: {\n      messages: messages,\n      max_tokens: options?.maxTokens,\n      temperature: options?.temperature,\n      top_p: options?.topP,\n      logit_bias: options?.logitBias,\n      user: options?.user,\n      n: options?.n,\n      stop: options?.stop,\n      presence_penalty: options?.presencePenalty,\n      frequency_penalty: options?.frequencyPenalty,\n      stream: options?.stream,\n      model: options?.model,\n    },\n  });\n}\n\nexport async function _getChatCompletionsDeserialize(\n  result: GetChatCompletions200Response | GetChatCompletionsDefaultResponse\n): Promise<ChatCompletions> {\n  if (isUnexpected(result)) {\n    throw result.body;\n  }\n\n  return {\n    id: result.body[\"id\"],\n    created: result.body[\"created\"],\n    choices: (result.body[\"choices\"] ?? []).map((p) => ({\n      message: !p.message\n        ? undefined\n        : { role: p.message?.[\"role\"], content: p.message?.[\"content\"] },\n      index: p[\"index\"],\n      finishReason: p[\"finish_reason\"],\n      delta: !p.delta ? undefined : { role: p.delta?.[\"role\"], content: p.delta?.[\"content\"] },\n    })),\n    usage: {\n      completionTokens: result.body.usage[\"completion_tokens\"],\n      promptTokens: result.body.usage[\"prompt_tokens\"],\n      totalTokens: result.body.usage[\"total_tokens\"],\n    },\n  };\n}\n\n/**\n * Gets chat completions for the provided chat messages.\n * Completions support a wide variety of tasks and generate text that continues from or \"completes\"\n * provided prompt data.\n */\nexport async function getChatCompletions(\n  context: Client,\n  messages: ChatMessage[],\n  deploymentId: string,\n  options: GetChatCompletionsOptions = { requestOptions: {} }\n): Promise<ChatCompletions> {\n  const result = await _getChatCompletionsSend(context, messages, deploymentId, options);\n  return _getChatCompletionsDeserialize(result);\n}\n\nexport function getCompletionsResult(body: Record<string, any>): Omit<Completions, \"usage\"> {\n  return {\n    id: body[\"id\"],\n    created: body[\"created\"],\n    choices: (body[\"choices\"] ?? []).map((p: ChoiceOutput) => ({\n      text: p[\"text\"],\n      index: p[\"index\"],\n      logprobs:\n        p.logprobs === null\n          ? null\n          : {\n              tokens: p.logprobs[\"tokens\"],\n              tokenLogprobs: p.logprobs[\"token_logprobs\"],\n              topLogprobs: p.logprobs[\"top_logprobs\"],\n              textOffset: p.logprobs[\"text_offset\"],\n            },\n      finishReason: p[\"finish_reason\"],\n    })),\n  };\n}\n\nexport function getChatCompletionsResult(\n  body: Record<string, any>\n): Omit<ChatCompletions, \"usage\"> {\n  return {\n    id: body[\"id\"],\n    created: body[\"created\"],\n    choices: (body[\"choices\"] ?? []).map((p: ChatChoiceOutput) => ({\n      message: !p.message\n        ? undefined\n        : { role: p.message?.[\"role\"], content: p.message?.[\"content\"] },\n      index: p[\"index\"],\n      finishReason: p[\"finish_reason\"],\n      delta: !p.delta ? undefined : { role: p.delta?.[\"role\"], content: p.delta?.[\"content\"] },\n    })),\n  };\n}\n"]}